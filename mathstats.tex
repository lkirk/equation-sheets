\documentclass[twocolumn]{amsart}
\usepackage[pdftex, a4paper, margin=0.7cm, nohead, nofoot]{geometry}
% \usepackage[utopia]{mathdesign}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{setspace}
\usepackage{scalefnt}
\usepackage{microtype}
\usepackage[absolute]{textpos}
\usepackage[compact]{titlesec}

\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\Beta}{\operatorname{Beta}}
\newcommand{\Cauchy}{\operatorname{Cauchy}}
\newcommand{\Unif}{\operatorname{Unif}}
\newcommand{\Poi}{\operatorname{Poi}}
\newcommand{\Bin}{\operatorname{Bin}}
\newcommand{\MSPE}{\operatorname{MSPE}}
\newcommand\iid{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny iid}}}{\sim}}}

% slight hack to make the independence symbol
% taken from here: https://tex.stackexchange.com/a/79436
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern3mu{#1#2}}}

\setlength{\parskip}{0pt}
\setlength{\parindent}{0pt}
\setlength{\TPHorizModule}{30mm}
\setlength{\TPVertModule}{\TPHorizModule}
\textblockorigin{10mm}{10mm} % start everything near the top-left corner
\titleformat{\section}{\centering\selectfont\bf}{}{0em}{}

\def\parsedate #1:20#2#3#4#5#6#7#8\empty{#6#7/#4#5/20#2#3\parsetime#8\empty}
\def\parsetime #1#2#3#4#5\empty{ #1#2:#3#4}
\def\moddate#1{\expandafter\parsedate\pdffilemoddate{#1}\empty}

% template taken from here: https://github.com/daleroberts/math-finance-cheat-sheet/

% More probability stuff
% https://wjgan.com/posts/latex.html

\begin{document}
\pagestyle{empty}
\thispagestyle{empty}
\setstretch{0.8}
\scalefont{0.8}

\section*{Expectation}
\begin{equation*}
  \E[g(X,Y)] = \sum_{y}\sum_{x}g(x,y)f_{X,Y}(x,y)
\end{equation*}
If $X\geq 0$ (continuous or discrete)
\begin{equation*}
  \E[X] = \int_{0}^{\infty}(1-F_{X}(x))~dx
\end{equation*}
Conditional expectation
\begin{equation*}
  \E[g(Y)|X=x] = \int g(y) f_{y|x}(y|x)dy
\end{equation*}
Law of iterated expectation
\begin{equation*}
  \E_{X}[E_{Y|X}\{g(Y)|X\}] = \E[g(Y)] = \sum_{x}\E[g(Y)|X=x]f_{X}(x)
\end{equation*}
\begin{equation*}
  \E[h(X)g(Y)] = \E[h(X)\E[g(Y)|X]]
\end{equation*}
If \(X \independent Y\), then
\begin{equation*}
  \E[g(Y) | X=x] = E[g(Y)]
\end{equation*}
\begin{equation*}
  \Var[X] = \E[X^{2}] - \E[X]^{2} = \Cov(X,X)
\end{equation*}
Special Case: Let $Y^{*} = I(Y=y)$
\begin{equation*}
  \E[Y^{*}] = \E[I(Y=y)] = \P(Y=y)
\end{equation*}
Special case cont
\begin{equation*}
  \E[Y^{*}] = \sum_{x}\E[Y^{*}|X=x]f_{X}(x) = \sum_{x}\P(Y=y|X=x)f_{X}(x) =
  \P(Y=y)
\end{equation*}

\section*{Basics}
\begin{equation*}
  \Cov[X,Y] = \E[(X-\E[X])(Y-\E[Y])] = \E[XY] - \E[X]\E[Y]
\end{equation*}
\begin{equation*}
  \rho(X,Y) = \frac{\Cov[X,Y]}{\sqrt{\Var[X]\cdot\Var[Y]}} = \frac{\Cov[X,Y]}{\sqrt{\sigma_{X}\sigma_{Y}}}
\end{equation*}

\section*{Expectation Algebra}
\begin{equation*}
  \E[X]=\E[X_{+}] - \E[X_{-}]~\text{if}~X_{+} \geq 0,~\E[|X|] = \E[X_{+}] + \E[X_{-}]~\text{if}~X_{-} \geq 0
\end{equation*}
\begin{equation*}
  \E[a+bX] = a + b\E[X],~\E[aX+bY+c] = a\E[X]+b\E[Y]+c
\end{equation*}
If \(X \independent Y\), then $\E[XY] = \E[X]\E[Y]$

\section*{Variance Algebra}
\begin{equation*}
  \Var[X + Y] = \Var[X] + 2\Cov[X, Y] + \Var[Y]
\end{equation*}
\begin{equation*}
  \Var[X - Y] = \Var[X] - 2\Cov[X, Y] + \Var[Y]
\end{equation*}
If \(X \independent Y\), then $\Var[X + Y] = \Var[X] + \Var[Y]$

If \(X \independent Y\), then $\Var[X - Y] = \Var[X] - \Var[Y]$
\begin{equation*}
  \Var[XY] = \E[X^2] \cdot \E[Y^2] - {(\E[X] \cdot \E[Y])}^2
\end{equation*}
\begin{equation*}
  \Var[X/Y] = \Var[X \cdot (1/Y)] = \Var[(1/Y) \cdot X]
\end{equation*}
\begin{equation*}
  \Var[aX+bY] = a^2\Var[X] + b^2 Var[Y] + 2ab\Cov[X,Y]
\end{equation*}

\section*{Correlation}
\begin{equation*}
  \Corr(X, Y) = \frac{\Cov(X, Y)}{\sqrt{\Var(X)}\sqrt{Var(Y)}}
\end{equation*}


\section*{Calc}
Chain rule: $f(g(x))' = f'(g(x))g'(x)$

Product rule: $(u \cdot v)' = u' \cdot v + u \cdot v'$

Quotient rule: $\left(\frac{f(x)}{g(x)}\right)' = \frac{f'(x)g(x) - f(x)g'(x)}{g(x)^2}$

Indeterminate Forms:
$\frac{0}{0}$, $\frac{\pm\infty}{\pm\infty}$, $\infty-\infty$, $0*\infty$,
$0^{0}$, $1^{\infty}$, $\infty^{0}$ \\
Determinate Forms \\
$\infty+\infty=\infty$, $-\infty-\infty=-\infty$, $0^{\infty}=0$,
$0^{-\infty}=\infty$, $\infty*\infty=\infty$

\section*{Series}
Exponential
\begin{equation*}
  \sum_{n=0}^{\infty}\frac{x^{n}}{n!} = e^{x}
\end{equation*}
Geometric
\begin{equation*}
  \sum_{k=0}^{\infty}{ar^{k}}=\frac{a}{1-r}~\text{where}~|r|<1
\end{equation*}
\begin{equation*}
  \sum_{k=1}^{\infty}{kr^{k-1}}=\frac{1}{(1-r)^{2}}~\text{where}~|r|<1
\end{equation*}
\begin{equation*}
  \sum_{k=2}^{\infty}{k(k-1)r^{k-2}}=\frac{-2}{(1-r)^{3}}~\text{where}~|r|<1
\end{equation*}
Arithmetico-geometric
\begin{equation*}
  \sum_{k=1}^{\infty}k(1-p)^{k-1}=\frac{1}{p^{2}}=p^{-2}
\end{equation*}
\begin{equation*}
  \sum_{k=2}^{\infty}k(k-1)(1-p)^{k-2}=\frac{2}{p^{3}}
\end{equation*}
Binomial
\begin{equation*}
  (x+y)^{n} = \sum_{k=0}^{n}\binom{n}{k}x^{k}y^{n-k}
\end{equation*}
\begin{equation*}
  n(x+y)^{n-1} = \sum_{k=1}^{n}k\binom{n}{k}x^{k-1}y^{n-k}
\end{equation*}
\begin{equation*}
  n(n-1)(x+y)^{n-2} = \sum_{k=2}^{n}k(k-1)\binom{n}{k}x^{k-2}y^{n-k}
\end{equation*}


\section*{Binomial Coefficient}
\begin{equation*}
  \binom{n}{k} = \binom{n}{n-k}
\end{equation*}
\begin{equation*}
  \binom{n}{k} = \binom{n-1}{k-1} \left( \frac{n}{k} \right)~\text{if}~k > 0
\end{equation*}
% There is a fantastic webpage here with many more identities
% https://web.archive.org/web/20190629193344/http://www.math.wvu.edu/~gould/
% https://proofwiki.org/wiki/Factors_of_Binomial_Coefficient
\begin{equation*}
  (n+1)\binom{r}{n+1} = r\binom{r-1}{n}
\end{equation*}
\begin{equation*}
  \binom{n}{r+a}\binom{r+a}{r} = \binom{n}{r}\binom{n-r}{a}
\end{equation*}
\begin{equation*}
  \frac{1}{k+1}\binom{n}{k} = \binom{n+1}{k+1}\frac{1}{n+1}
\end{equation*}

\section*{Distribution Functions}
\begin{equation*}
  \lim_{x\to-\infty}F(x)=0~\lim_{x\to\infty}F(x)=1
\end{equation*}
\begin{equation*}
  F(x) \leq F(y)~\text{if}~x<y
\end{equation*}
\begin{equation*}
  \lim_{h \downarrow 0}F(x+h)=F(x)
\end{equation*}
\begin{equation*}
  F(x)=\int_{-\infty}^{x}f(t)~dt~\text{where}~f(t)\geq
  0~\text{and}\int_{-\infty}^{\infty}f(t)~dt = 1
\end{equation*}
\begin{equation*}
  F_{X,Y}(x,y)=\P(X\leq x,Y\leq y)
\end{equation*}
\begin{equation*}
  F_{\boldsymbol{X}}(\boldsymbol{x})=\P(\boldsymbol{X}\leq\boldsymbol{x}) =
  \P(X_{1}\leq x_{1},\ldots,X_{k}\leq x_{k})
\end{equation*}
\begin{equation*}
  F_{\boldsymbol{X}}(\boldsymbol{x})=\int_{-\infty}^{x_{1}}\dotsi
  \int_{-\infty}^{x_{k}}f(t_{1},\cdots,t_{k})~dt_{1}\dotsm dt_{k}
\end{equation*}
\begin{equation*}
  \P(a < X \leq b, c < Y \leq d) = F(b,d) - F(a,d) - F(b,c) + F(a,c)
\end{equation*}
Joint Density
\begin{equation*}
  F(x,y) = \P(X \leq x,Y \leq y) = \int_{-\infty}^{x}\left\{ \int_{-\infty}^{y}
    f(u,v)~dv\right\}~du
\end{equation*}

\section*{Random Variables}
\begin{equation*}
  X: \Omega \rightarrow \mathbb{R}
\end{equation*}
\begin{equation*}
  \{\omega \in \Omega : X(\omega) \leq x\} \in \mathcal{F}
\end{equation*}
\begin{equation*}
  F(x) = \P(X \leq x) = \P(\{\omega : X(\omega) \leq x\})
\end{equation*}
\begin{equation*}
  \P(X>x) = 1-F(x)
\end{equation*}
\begin{equation*}
  \P(x<X\leq y) = F(y) - F(x)
\end{equation*}
\begin{equation*}
  \P(X<x) = \lim_{n\to \infty}F(x - \frac{1}{n})
\end{equation*}
\begin{equation*}
  \P(X=x) = \P(X\leq x) - \P(X < x) = F(x)-\lim_{n\to \infty}F(x-\frac{1}{n})
\end{equation*}

\section*{PDF}
Joint PDF
\begin{equation*}
  f(x,y) = \frac{\partial^{2}}{\partial y \partial x}F(x,y)
\end{equation*}
Marginal PDF
\begin{equation*}
  f_{X}(x) = \int_{-\infty}^{+\infty} f(x,v)~dv,~f_{Y}(y) = \int_{-\infty}^{+\infty} f(u,y)~du
\end{equation*}

\section*{Convolution}
Let $Z=X+Y$
\begin{equation*}
  f_{Z}(z)=\sum_{x}f_{X}(x)f_{Y|X}(z-x|x) = \sum_{y}f_{Y}(y)f_{X|Y}(z-y|y)
\end{equation*}
If \(X \independent Y\), then
\begin{equation*}
  f_{Z}(z)=\sum_{x}f_{X}(x)f_{Y}(z-x) = \sum_{y}f_{Y}(y)f_{X}(z-y)
\end{equation*}

\begin{equation*}
  f_{Z}(z)=\int_{-\infty}^{\infty}f_{X,Y}(x,z-x)~dx = \int_{-\infty}^{\infty}f_{X,Y}(z-y,y)~dy
\end{equation*}
If \(X \independent Y\), then
\begin{equation*}
  f_{Z}(z)=\int_{-\infty}^{\infty}f_{X}(x)f_{Y}(z-x)~dx = \int_{-\infty}^{\infty}f_{X}(z-y)f_{Y}(y)~dy
\end{equation*}

\section*{Bivariate Normal}
\begin{equation*}
  \Sigma = \Cov(\boldsymbol{X},\boldsymbol{X})
  = \begin{pmatrix}
    \sigma_{1}^{2}&\Cov(X_{1},X_{2})\\
      \Cov(X_{1},X_{2})&\sigma_{2}^{2}
  \end{pmatrix}
  = \begin{pmatrix}
    \sigma_{1}^{2}&\rho\sigma_{1}\sigma_{2}\\
      \rho\sigma_{1}\sigma_{2}&\sigma_{2}^{2}
  \end{pmatrix}
\end{equation*}

% \begin{equation*}
%   f_{\boldsymbol{X}}(\boldsymbol{x}) =
%   \frac{1}{2\pi\sigma_{1}\sigma{2}\sqrt{1-\rho^{2}}}
%   \exp\left\{-\frac{1}{2(1-\rho^{2})}\left( \frac{x_{1} - \mu_{1}}{\sigma_{1}} \right)^{2}
%     - 2\rho \left(\frac{x_{1} - \mu_{1}}{\sigma_{1}} \frac{x_{2} -
%         \mu_{2}}{\sigma_{2}}\right)
%     + \left(\frac{x_{2} - \mu_{2}}{\sigma_{2}}\right)^{2} \right\}
% \end{equation*}

\begin{equation*}
  \Cov(X_{1},X_{2}) = 0 \iff X_{1} \independent X_{2}
\end{equation*}

\begin{equation*}
  \boldsymbol{U} = \boldsymbol{v} + C\boldsymbol{X} \sim
  \mathcal{N}(\boldsymbol{v} + C\boldsymbol{\mu},C\Sigma C)
\end{equation*}

\begin{equation*}
  X_{2} | X_{1} = x_{1} \sim \mathcal{N}(\mu_{2} + \rho
  \frac{\sigma_{2}}{\sigma{1}}(x_{1} - \mu_{1}),\sigma_{2}^{2}(1-\rho^{2}))
\end{equation*}

\begin{equation*}
  \E[X_{2} | X_{1}] = \mu_{2} + \rho \frac{\sigma_{2}}{\sigma{1}}(X_{1} - \mu_{1})
\end{equation*}


\section*{Distributions arising from $\mathcal{N}$}
Gamma distribution
\begin{equation*}
  \Gamma\left(\frac{1}{2},\frac{k}{2}\right) \rightarrow \chi_{k}^{2}
  \rightarrow t_{k} \stackrel{D}{=}
  \frac{\mathcal{N}(0,1)}{\sqrt{\chi_{k}^{2}/k}} \rightarrow F_{m,k}
  \stackrel{D}{=} \frac{\chi_{m}^{2}/m}{\chi_{k}^{2}/k}.
  (F_{1,k} \stackrel{D}{=} t_{k}^{2})
\end{equation*}

\begin{equation*}
  X\sim\Gamma(\lambda,p) \iff \lambda X \sim \Gamma(1,p)
  \stackrel{c>0}{\implies} cX\sim \Gamma(\frac{\lambda}{c},p)
\end{equation*}

\begin{equation*}
  X_{j}\sim \Gamma(\lambda, p_{j}), j=1,\ldots,k, X_{1},\ldots,X_{k}
  \independent \implies \sum_{j=1}^{k}X_{j} \sim \Gamma(\lambda, \sum_{j=1}^{k}p_{j})
\end{equation*}

\begin{equation*}
  \begin{cases}
    X_{1} & \sim \Gamma(\lambda, p) \\
    X_{2} & \sim \Gamma(\lambda, q) \\
    X_{1} & \independent X_{2}
  \end{cases}
  \implies
  \begin{cases}
    Y_{1} = X_{1} + X_{2} & \sim \Gamma(\lambda, p + q) \\
    Y_{2} = \frac{X_{1}}{X_{1} + X_{2}} & \sim \Beta(p, q) \\
    Y_{1} & \independent Y_{2}
  \end{cases}
\end{equation*}

$\chi_{k}^{2}$ distribution (k degrees of freedom)

\begin{equation*}
  Z_{1},\ldots,Z_{k}\iid\mathcal{N}(0,1) \implies V =
  Z^{2}_{1}+\cdots+Z^{2}_{k} \sim \chi_{k}^{2}
\end{equation*}

\begin{equation*}
  f_{V}(v) = \frac{v^{(k-2)/2}e^{-v/2}}{2^{k/2}\Gamma(k/2)} \sim
  \Gamma\left(\frac{1}{2},\frac{k}{2}\right), v \geq 0
\end{equation*}

\begin{equation*}
  \mathcal{N}(0,1)^{2} \sim \chi_{1}^{2} \stackrel{D}{=} \Gamma\left(\frac{1}{2},\frac{1}{2}\right)
\end{equation*}

\begin{equation*}
  \chi_{2}^{2} = \Gamma\left(\frac{1}{2},1\right) = \Exp\left(\frac{1}{2}\right)
\end{equation*}

$t_{k}$ distribution (k degrees of freedom)\\
if $Z \sim \mathcal{N}(0, 1),~V \sim \chi_{k}^{2},~Z \independent V$ then $Q = \frac{Z}{\sqrt{V/k}} \sim t_{k}$
\begin{equation*}
  f_{Q}(q) = c_{1}{(1+\frac{q^{2}}{k})}^{-(k+1)/2}, q \in \mathbb{R}
\end{equation*}
where \(c_{1}>0\) is a constant: $\frac{1}{1 + q^{2}} \sim \Cauchy(0,1) \stackrel{D}{=} t_{1}$

\begin{equation*}
  t_{k} \to \mathcal{N}(0,1) \text{ as } k\to\infty
\end{equation*}

\textbf{F Distribution} \\
if $V \sim \chi^{2}_{m},~W \sim \chi^{2}_{k},~V \independent W$
then $S = \frac{V/m}{W/k} \sim F_{m,k}$
\begin{equation*}
  f_{S}(s) = c_{2}s^{(m-2)/2}(1+\frac{m}{k}s)^{-(k+m)/2},s>0
\end{equation*}
where $c_{2} > 0$ is a constant

Special case:
if \(m=1\), then $F_{m,k}=F{1,k} \stackrel{D}{=} t^{2}_{k}$

\section{Estimators}
Estimate $\mu$: $\overline{X}_{n} = \frac{1}{n}\sum_{i=1}^{n}X_{i}$

Estimate $\sigma^{2}$: $S_{n}^{2} = \frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X}_{n})^{2}$

Suppose $\{X_{1},\ldots,X_{n}\}\iid\mathcal{N}(\mu,\sigma^{2})$, then
\begin{equation*}
  \sqrt{n}\frac{\overline{X}_{n} - \mu}{\sigma}\sim\mathcal{N}(0,1),~~
  \overline{X}_{n}\independent S_{n}^{2},~~
  \E[S_{n}^{2}] = \sigma^{2},~~\frac{(n-1)S_{n}^{2}}{\sigma^{2}}\sim \chi^{2}_{n-1}
\end{equation*}
And
  $\sqrt{n}\frac{\overline{X}_{n} - \mu}{\sigma}\sim \mathcal{N}(0,1),~~
  \sqrt{n}\frac{\overline{X}_{n} - \mu}{S_{n}}\sim t_{n-1}$

\section{Sampling}
Inverse Transform

Let $U\sim\Unif(0, 1)$, $F$ be some continuous distribution. Assume $F(x)$ is strictly
$\uparrow$ in x, then: $X=F^{-1}(U)\sim F$

Discrete

Let $U\sim\Unif(0, 1)$, suppose $X\in\{0,1,2,\ldots\} \sim F(x)$. We wish to
generate X. For $k=0,1,2,\ldots$, define $Y=k\iff F(k-1)<U\leq F(k)$, then
$Y\sim F$

Rejection

Suppose $X\sim F_{X}$ with a known PDF: $f_{X}$. Let $U\sim\Unif(0,1)$,
$Z\independent U$, $Z\sim f_{Z}(z)$, where the pdf is known. $\exists$ a known
constant $a > 0$, such that $f_{X}(z)\leq a f_{Z}(z) \forall z$

Event $E=\{aUf_{Z}(Z)\leq f_{X}(Z)\}$. $P(Z\leq x | E) = F_{X})(x)$


\section{Generating Functions -- PGF}
(non-negative integer-valued RVs) -- No finiteness guarantee
\begin{equation*}
  G_{X}(s) = \E(s^{X}) = \sum_{j=0}^{\infty}s^{j}\P(X=j) = \sum_{j=0}^{\infty}s^{j}f_{X}(j)
\end{equation*}
Factorial moments
\begin{equation*}
  \E[X(X-1)\cdots(X-k+1)] = G^{(k)}(1)
\end{equation*}

\begin{equation*}
  X \independent Y \implies G_{X+Y}(s) = G_{X}(s)G_{Y}(s)
\end{equation*}

If $X_{1},X_{2},\ldots$ iid  with PGF $G_{X}$ and $N (\geq 0)$ is a rv w/ PGF
$G_{N}$, then the sum $S_{N} = X_{1}+\cdots+X_{N}$ has the PGF:
\begin{equation*}
  G_{S_{N}}(s) = G_{N}(G_{X}(t))
\end{equation*}
\begin{equation*}
  G_{X_{1},X_{2}}(s_{1},s_{2}) = \E[s_{1}^{X_{1}}s_{2}^{X_{2}}]
\end{equation*}
If $X_{1} \independent X_{2}$:
\begin{equation*}
  G_{X_{1},X_{2}}(s_{1},s_{2}) = G_{X_{1}}(s_{1})G_{X_{2}}(s_{2})
\end{equation*}


\section{Generating Functions -- MGF}
(General RVs)
\begin{equation*}
  M_{X}(t) = \E(e^{tX}) = G_{X}(e^{t}) = \sum_{j=0}^{\infty}e^{tj}f_{X}(j) =
  \int e^{tx}f_{X}(x)~dx
\end{equation*}
\begin{equation*}
  M_{X_{1},X_{2}}(t_{1},t_{2}) = \E[e^{t_{1}X_{1}}e^{t_{2}X_{2}}] = G_{X_{1},X_{2}}(e^{t_{1}},e^{t_{2}})
\end{equation*}
If $X_{1} \independent X_{2}$:
\begin{equation*}
  M_{X_{1},X_{2}}(t_{1},t_{2}) = M_{X_{1}}(t_{1})M_{X_{2}}(t_{2})
\end{equation*}
For $S_{n} = X_{1}+\cdots+X_{n}$, if $\left\{X_{i}\right\}^{n}_{i=1}$ are
independent, then:
\begin{equation*}
  M_{S_{n}}(t) = \prod_{k=1}^{n}M_{X_{k}}(t)
\end{equation*}
In addition, if $\left\{X_{i}\right\}^{n}_{i=1}$ have the same distribution as
X, then:
\begin{equation*}
  M_{S_{n}}(t) = (M_{X}(t))^{n}
\end{equation*}

For $Y=a+bX$, if $M_{X}$ exists:
\begin{equation*}
  M_{Y}(t) = \E[e^{tY}] = \E[e^{ta+tbX}] = e^{ta}\E[e^{tbX}] = e^{ta}M_{X}(tb)
\end{equation*}

If $|M_{X}(t)| < \infty$ for $t \in (-a,a)$, where $a>0$ is some constant, then:
\begin{equation*}
  M_{X}^{(k)}(0) = \E[X^{k}]
\end{equation*}

\section*{Stein's Lemma}
If \(g(z)\) is differentiable and \(\lim\limits_{z\to-\infty} g(z)\phi(z) = 0\) and \(\lim\limits_{z\to+\infty} g(z)\phi(z) = 0\) \\
Then for \(Z\sim\mathcal{N}(0, 1)\): $\E[Z g(Z)] = \Cov(Z, g(Z)) = \E[g'(Z)]$

\textbf{Applications}\\
For \(Z\sim\mathcal{N}(0,1)\): $\E[Z^{2k}] = (2k - 1)!!$
if \(\lim\limits_{z\to\pm\infty} m(\mu + \sigma z)\phi(z)=0\), then: $Cov(X,m(X)) = \sigma^{2}\E[m'(X)]$

\section*{CLT}
Suppose \(\{X_{1},\ldots,X_{n}\} \iid X\), with \(\mu = \E[X]\) and \(\sigma^{2}
= \Var(X) \in (0, \infty)\). Define
\(Z_{n}=\sqrt{n}\frac{\overline{X}_{n}-\mu}{\sigma}\). Then: $Z_{n} \overset{D}{\longrightarrow} Z\sim\mathcal{N}(0,1)$

\section*{LLN}
Suppose \(\{X_{1},\ldots,X_{n}\} \iid X\), with \(\mu = \E[X]\). Then: $\overline{X}_{n} \overset{D}{\longrightarrow} \mu$

\section*{MGFs}
Gamma: $\Gamma(\lambda,\alpha) =
\left(\frac{\lambda}{\lambda-t}\right)^{\alpha}, t < \lambda$
Binomial: $(q+pe^{t})^{n}$
Poisson: $\exp\{\lambda(e^{t} - 1)\}$
Geometric: $\frac{pe^{t}}{1-(1-p)e^{t}}, t<-\ln(1-p)$
Exponential: $\frac{\lambda}{\lambda-t}, t<\lambda$

\section*{EDF}
$F(x)=P(X\leq x)=\E[I(X\leq x)]$, $\hat{\mathbb{F}}_{n}(x) =
\frac{1}{n}\sum_{i=1}^{n}I(X_{i}\leq x) = \frac{1}{n}N$, where
$N\sim\Bin(n,F(x))$. $\E[\hat{\mathbb{F}}_{n}(x)] = F(x)$,
$\Var(\hat{\mathbb{F}}_{n}(x)) = \frac{1}{n}F(x)(1-F(x))$

LLN: $\hat{\mathbb{F}}_{n}(x) \overset{D}{\to} F(x)$, CLT: If $0<F(x)<1$, then
$\sqrt{n}\frac{\hat{\mathbb{F}}_{n}(x) - F(x)}{\sqrt{F(x)(1-F(x))}}$

\section*{Inequalities}
Chebychev: $\P(|X|\geq a) \leq \frac{\E(|X|^r)}{a^r}\text{, for any }a>0,~r>0$

Markov: $\P(|X|\geq a) \leq \frac{E(|X|)}{a}$

Chernoff: $\P(X\geq a)\leq \inf_{s>0}\frac{\E(M_{X}(s))}{e^{sa}}=\inf_{s>0}\frac{M_{X}(s)}{e^{sa}}$

\section*{Convergence}
Distribution
\begin{equation*}
  \lim_{n\to\infty}F_{X_{n}}(x)=F_{X}(x)\forall x \text{ where }F_{X}\text{ is continuous}
\end{equation*}
Continuity Theorem: $\lim_{n\to\infty}M_{X_{n}}(t)=F_{X}(t)\forall t \in (-a,a) \text{ with } a>0$
\begin{equation*}
  \lim_{n\to\infty}P_{X_{n}}(k)=P_{X}(k)\forall k \in \{0, 1, 2, \ldots\}
\end{equation*}
Probability
\begin{equation*}
  \lim_{n\to\infty}\P(|X_n-X|\geq \epsilon) = 0~\forall\epsilon > 0
\end{equation*}
Slutsky's Theorem (If $Z_n\overset{D}{\longrightarrow}Z$ and
$U_n\overset{D}{\longrightarrow}U$, where $U$ const, $U\neq 0$ div)
\begin{equation*}
  Z_n+U_n\overset{D}{\longrightarrow}Z+C,~Z_n-U_n\overset{D}{\longrightarrow}Z-C,~Z_n/U_n\overset{D}{\longrightarrow}Z/C,~Z_{n}U_{n}\overset{D}{\longrightarrow}ZC
\end{equation*}
Rth mean (also, if $r>s\geq 1 \text{, then } X_n\overset{r}{\longrightarrow}
X\implies X_n\overset{s}{\longrightarrow}X$)
\begin{equation*}
  \lim_{n\to\infty}\E(|X_n-X|^r)=0
\end{equation*}

\section*{Prediction}
MSPE Pred and its MSPE $\hat{g}(\boldsymbol{X}) = \E(Y|\boldsymbol{X}) = \mu(\boldsymbol{X}),~\MSPE(\hat{g})=\E(\Var(Y|\boldsymbol{X}))$

MSPE Model Decomposition:
\begin{equation*}
  \MSPE(g) = \E[(Y-g(\boldsymbol{X}))^2] = \E[(Y-\mu(\boldsymbol{X}))^2] + \E[(\mu(\boldsymbol{X})-g(\boldsymbol{X}))^2]
\end{equation*}

Variance Decomposition: $\Var(Y) = \Var(\E[Y|\boldsymbol{X}]) + \E[\Var(Y|\boldsymbol{X})]$

Model Decomposition: $Y=\E[Y|\boldsymbol{X}] + (Y - \E[Y|\boldsymbol{X}]) = \mu(\boldsymbol{X}) + \epsilon$

Linear Pred: $\hat{g}_L(X)=\mu_{Y}+\rho\sigma_{Y}\frac{(X-\mu_X)}{\sigma_X},~MSPE(\hat{g}_L)=(1-\rho^2)\sigma^2_Y$

\section*{Poisson Process}

Number of events in interval $[0,t]$: $N(t)\sim\Poi(\lambda t)$

Num events in interval (indep if ivls non-overlapping) $(s, t]$: $N(t)-N(s)$

Stationary Incr:
$N(t+h)-N(t)\overset{D}{=}N(h)-N(0)\overset{D}{=}N\sim\Poi(\lambda h),~t\geq 0$

Arrival t: $T_n=\sum_{i=1}^n X_j \sim \Gamma(\lambda, n)\implies \E[T_n]=n/\lambda,~\Var(T_n)=n/\lambda^2$

Arrival t: $\P(T_n > t) = \P(\Gamma(\lambda, n) > t) =
\sum_{k=0}^{n-1}\frac{(\lambda t)^k}{k!}e^{-\lambda t}$

Inter Arrival t: $X_1,X_2,\ldots\iid\Exp(\lambda)$, $X_j=T_j-T_{j-1},~j\geq 1$

\end{document}