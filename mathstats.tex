\documentclass[twocolumn]{amsart}
\usepackage[pdftex, a4paper, margin=0.7cm, nohead, nofoot]{geometry}
% \usepackage[utopia]{mathdesign}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{setspace}
\usepackage{scalefnt}
\usepackage{microtype}
\usepackage[absolute]{textpos}
\usepackage[compact]{titlesec}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\Beta}{\operatorname{Beta}}

% slight hack to make the independence symbol
% taken from here: https://tex.stackexchange.com/a/79436
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern3mu{#1#2}}}

\setlength{\parskip}{0pt}
\setlength{\parindent}{0pt}
\setlength{\TPHorizModule}{30mm}
\setlength{\TPVertModule}{\TPHorizModule}
\textblockorigin{10mm}{10mm} % start everything near the top-left corner
\titleformat{\section}{\centering\selectfont\bf}{}{0em}{}

\def\parsedate #1:20#2#3#4#5#6#7#8\empty{#6#7/#4#5/20#2#3\parsetime#8\empty}
\def\parsetime #1#2#3#4#5\empty{ #1#2:#3#4}
\def\moddate#1{\expandafter\parsedate\pdffilemoddate{#1}\empty}

% template taken from here: https://github.com/daleroberts/math-finance-cheat-sheet/

% More probability stuff
% https://wjgan.com/posts/latex.html

\begin{document}
\pagestyle{empty}
\thispagestyle{empty}
\setstretch{0.8}
\scalefont{0.8}

\begin{center}
\textbf{MathStats}
\vskip0.2em
\end{center}

\section*{Densities}
\begin{equation*}
  f_{Y|X}(y|x) = \frac{f(x,y)}{f_{X}(x)} = \frac{f(x,y)}{\int_{-\infty}^{\infty}f(x,y)\,dy}
\end{equation*}
\begin{equation*}
  F_{Y|X}(y|x) = \int_{-\infty}^{y} \frac{f(x,v)}{f_{X}(x)}\,dv
\end{equation*}

\section*{Expectation}
\begin{equation*}
  \E[x^n] = \int_{-\infty}^{\infty} x^n f_{X}(x)
\end{equation*}
\begin{equation*}
  \E[Y] = \int_{-\infty}^{\infty}\E[Y|X=x]f_{X}(x)\,dx
\end{equation*}
\begin{equation*}
  \E[X^n] = \sum_{x:f(x)>0}x^{n}f(x)
\end{equation*}
Conditional expectation
\begin{equation*}
  \E[Y|X=x] = \int y f_{y|x}(y|x)dy
\end{equation*}
\begin{equation*}
  \E[g(Y)|X=x] = \int g(y) f_{y|x}(y|x)dy
\end{equation*}
Law of iterated expectation
\begin{equation*}
  \E_{X}[E_{Y|X}\{g(Y)|X\}] = E[g(Y)]
\end{equation*}
If \(X \independent Y\), then
\begin{equation*}
  E[g(Y) | X=x] = E[g(Y)]
\end{equation*}
% or
% \begin{equation*}
%   \E_
% \end{equation*}
% or
% \begin{equation*}
  
% \end{equation*}
Not well defined (cauchy)
\begin{equation*}
  E[X]=E[X_{+}] - E[X_{-}] = \infty - \infty
\end{equation*}
Well defined (cauchy)
\begin{equation*}
  E[|X|] = \infty
\end{equation*}



\section*{Basics}

\begin{equation*}
  \Cov[X,Y] = \E[(X-\E[X])(Y-\E[Y])] = \E[XY] - \E[X]\E[Y]
\end{equation*}
\begin{equation*}
  \rho(X,Y) = \frac{\Cov[X,Y]}{\sqrt{\Var[X]\cdot\Var[Y]}} = \frac{\Cov[X,Y]}{\sqrt{\sigma_{X}\sigma_{Y}}}
\end{equation*}

\section*{Expectation Algebra}
\begin{equation*}
  \E[x^n] = \int_{-\infty}^{\infty} x^n f_{x}(x)
\end{equation*}

\section*{Variance Algebra}
\begin{equation*}
  \Var[X + Y] = \Var[X] + 2\Cov[X, Y] + \Var[Y]
\end{equation*}
\begin{equation*}
  \Var[X - Y] = \Var[X] - 2\Cov[X, Y] + \Var[Y]
\end{equation*}
\begin{equation*}
  \Var[XY] = \E[X^2] \cdot \E[Y^2] - {(\E[X] \cdot \E[Y])}^2
\end{equation*}
\begin{equation*}
  \Var[X/Y] = \Var[X \cdot (1/Y)] = \Var[(1/Y) \cdot X]
\end{equation*}
\begin{equation*}
  \Var[X] = \Cov(X, X) = E[X^2] - {E[X]}^2
\end{equation*}
\begin{equation*}
  \Var[aX+bY] = a^2\Var[X] + b^2 Var[Y] + 2ab\Cov[X,Y]
\end{equation*}

\section*{Correlation}
\begin{equation*}
  \Corr(X, Y) = \frac{\Cov(X, Y)}{\sqrt{\Var(X)}\sqrt{Var(Y)}}
\end{equation*}


\section*{Calc}
By parts
\begin{equation*}
  \int u\,dv = uv-\int v\,du
\end{equation*}
Chain rule
\begin{equation*}
  f(g(x))' = f'(g(x))g'(x)
\end{equation*}
Product rule
\begin{equation*}
  (u \cdot v)' = u' \cdot v + u \cdot v'
\end{equation*}
\begin{equation*}
  (\frac{f(x)}{g(x)})' = \frac{f'(x)g(x) - f(x)g'(x)}{g(x)^2}
\end{equation*}
Jacobian
\begin{equation*}
\mathbb{J}=\begin{vmatrix}
\frac{\partial x}{\partial u}\frac{\partial x}{\partial v} \\
\\
\frac{\partial y}{\partial u}\frac{\partial y}{\partial v} \\
\end{vmatrix}
\end{equation*}
\begin{equation*}
  \iint_{A}g(x,y)\,dx\,dy = \iint_{B}g(x(u,v),y(u,v))|J(u,v)|\,du\,dv
\end{equation*}

\section*{Distributions}
\textbf{Distributions arising from the Normal}
\begin{equation*}
  \Gamma(\frac{1}{2},\frac{k}{2})
  \rightarrow \chi_{k}^{2}
  \rightarrow t_{k}
  \stackrel{D}{=} \frac{\mathcal{N}(O, 1)}{\sqrt{\chi_{k}^{2}/k}}
  \rightarrow F_{m,k}
  \stackrel{D}{=} \frac{\chi_{m}^{2}/m}{\chi_{k}^{2}/k}
\end{equation*}
\begin{equation*}
  F_{1,k} \stackrel{D}{=} t_{k}^{2}
\end{equation*}
\textbf{Gamma}
\begin{equation*}
  X \sim \Gamma(\lambda,p) \Longleftrightarrow \lambda X \sim \Gamma(1, p)
\end{equation*}
\begin{equation*}
  X \sim \Gamma(\lambda,p) \overset{c>0}{\Longrightarrow} c X \sim \Gamma(\frac{\lambda}{c},p)
\end{equation*}
\begin{equation*}
  Theorem (\Gamma(\lambda, p) + \Gamma(\lambda, q) \overset{\independent}{=} \Gamma(\lambda,p+q)):
\end{equation*}
\begin{equation*}
  \begin{cases}
    X_{1} \sim \Gamma(\lambda,p) \\
    X_{2} \sim \Gamma(\lambda,q) \\
    X_{1} \independent X_{2}
  \end{cases}
  \Longrightarrow
  \begin{cases}
    Y_{1} = X_{1} + X_{2} \sim \Gamma(\lambda, p + q) \\
    Y_{2} = \frac{X_{1}}{X_{1} + X_{2}} \sim \Beta(p, q) \\
    Y_{1} \independent Y_{2}
  \end{cases}
\end{equation*}
\textbf{Chi Squared}\\
Definition \\
if \(\{Z_{1},\ldots,Z_{k}\} \overset{i.i.d.}{\sim} \mathcal{N}(0,1)\), then the distribution of
\begin{equation*}
  V = Z_{1}^{2}+\cdots+Z_{k}^{2}
\end{equation*}
is called the \(\chi_{k}^{2}\) distribution with \(k\) degrees of freedom
PDF of \(chi_{k}^{2}\):
\begin{equation*}
  f_{V}(v) = \frac{v^{(k-2)/2}e^{-v/2}}{2^{k/2}\Gamma(k/2)} \sim \Gamma(\frac{1}{2},\frac{k}{2}), v\geq0
\end{equation*}
Chi squared distribution with k=2 is a gamma/exp with the following params
\begin{equation*}
  \chi_{2}^{2} = \Gamma(\frac{1}{2},1) = \Exp(\frac{1}{2})
\end{equation*}
\textbf{T Distribution} \\
if
\begin{equation*}
  Z \sim \mathcal{N}(0, 1)
\end{equation*}
\begin{equation*}
  V \sim \chi_{k}^{2}
\end{equation*}
\begin{equation*}
  Z \independent V
\end{equation*}
then
\begin{equation*}
  Q = \frac{Z}{\sqrt{\frac{V}{k}}}
\end{equation*}
which is the Student's t distribution with k degrees of freedom \\
PDF of \(t_{k}\):
\begin{equation*}
  f_{Q}(q) = c_{1}{(1+\frac{q^{2}}{k})}^{-(k+1)/2}, q \in \mathbb{R}
\end{equation*}
where \(c_{1}>0\) is a constant

\textbf{F Distribution} \\
\begin{equation*}
  V \sim \chi^{2}_{m}
\end{equation*}
\begin{equation*}
  W \sim \chi^{2}_{k}
\end{equation*}
\begin{equation*}
  V \independent W
\end{equation*}
Then the distribution of
\begin{equation*}
  S = \frac{V/m}{W/k}
\end{equation*}
is called the Fisher distribution with m (numerator) and k (denominator) degrees of freedom
\(S\) has the PDF:
\begin{equation*}
  f_{S}(s) = c_{2}s^{(m-2)/2}(1+\frac{m}{k}s)^{-(k+m)/2},s>0
\end{equation*}
Special case:
if \(m=1\), then
\begin{equation*}
  F_{m,k}=F{1,k} \stackrel{D}{=} t^{2}_{k}
\end{equation*}


\textbf{Bivariate Normal} \\
if
\begin{equation*}
  \textbf{X} \sim \mathcal{N}(\mu, \Sigma)
\end{equation*}
then a bivariate normal vector \(\textbf{X} = (X_{1},X_{2})^T\) has the pdf
\begin{equation*}
  f_{\textbf{X}}(\textbf{x}) = \frac{1}{2\pi\sqrt{\det(\Sigma)}}e^{-\frac{1}{2}{(\textbf{x}-\mu)}^{T}\Sigma^-1(\textbf{x}-\mu)}
\end{equation*}

\begin{equation*}
  \Sigma =
  \begin{pmatrix}
    \sigma_{1}^2 & \rho \sigma_{1} \sigma_{2} \\
    \rho \sigma_{1} \sigma_{2} & \sigma_{2}^2
  \end{pmatrix}
\end{equation*}


linear transforms
\begin{equation*}
  C\textbf{X} + \textbf{v} \sim \mathcal{N}(C\mu, C \Sigma C^T)
\end{equation*}
\begin{equation*}
  \E[\textbf{X}] = C\boldsymbol{\mu}
\end{equation*}

For \(\textbf{X} = (X_{1},X_{2})^T \sim \) bivariate Normal, if \(\rho = 0\), then
\begin{equation*}
  cov(X_{1},X_{2}) = 0 \Longleftrightarrow X_{1} \independent X_{2}
\end{equation*}

Conditional pdf
\begin{equation*}
  X_{2} | X_{1} = x_{1} \sim \mathcal{N}(\mu_{2} + \rho\frac{\sigma_{2}}{\sigma_{1}}(x_{1}-\mu_{1}),\sigma_{2}^{2}(1-\rho^{2}))
\end{equation*}

And by definition
\begin{equation*}
  f_{X_{2}|X_{1}}(x_{2}|x_{1}) = \frac{f_{X_{1},X_{2}}(x_{1},x_{2})}{f_{X_{1}}(x_{1})}
\end{equation*}

Regression line
\begin{equation*}
  \E[X_{2}|X_{1}] = \mu_{2} + \rho\frac{\sigma_{2}}{\sigma_{1}}(X_{1} - \mu_{1})
\end{equation*}

\textbf{Multivariate Normal}
\begin{equation*}
  \textbf{X} =
  \begin{pmatrix}
    \boldsymbol{X_{1}} \\
    \boldsymbol{X_{2}}
  \end{pmatrix}
  \sim
  \mathcal{N}
  \begin{pmatrix}
    \begin{pmatrix}
      \mu_{1} \\
      \mu_{2}
    \end{pmatrix}
    \begin{pmatrix}
      \Sigma_{11} & \Sigma_{12} \\
      \Sigma_{21} & \Sigma_{22}
    \end{pmatrix}
  \end{pmatrix}
\end{equation*}

\begin{equation*}
  \boldsymbol{X_{2}} | \boldsymbol{X_{1}} \sim \mathcal{N}(\mu_{2}+\Sigma_{21}\Sigma_{11}^{-1}(\boldsymbol{X_{1}} - \mu_{1}),\Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})
\end{equation*}

\section*{Convolution}
If \(X \independent Y\), then \(Z = X + Y\) has the PDF
\begin{equation*}
  f_{Z}(z) = \int_{-\infty}^{+\infty} f_{X}(x) f_{Y}(z-x) dx = \int_{-\infty}^{+\infty} f_{X}(z-y) f_{Y}(y) dy
\end{equation*}

\section*{Stein's Lemma}
If:\\
function \(g(z)\) is differentiable\\
and \(\lim\limits_{z\to-\infty} g(z)\phi(z) = 0\) and \(\lim\limits_{z\to+\infty} g(z)\phi(z) = 0\) \\
Then for \(Z\sim\mathcal{N}(0, 1)\):
\begin{equation*}
  \E[Z g(Z)] = \Cov(Z, g(Z)) = \E[g'(Z)]
\end{equation*}
\textbf{Applications}\\
For \(Z\sim\mathcal{N}(0,1)\):
\begin{equation*}
  \E[Z^{2k}] = (2k - 1)!!
\end{equation*}
if \(\lim\limits_{z\to\pm\infty} m(\mu + \sigma z)\phi(z)=0\), then:
\begin{equation*}
  Cov(X,m(X)) = \sigma^{2}\E[m'(X)]
\end{equation*}

\section*{CLT}
Suppose \(\{X_{1},\ldots,X_{n}\} \overset{i.i.d.}{\sim} X\), with \(\mu = \E[X]\) and \(\sigma^{2} = \Var(X) \in (0, \infty)\). Define \(Z_{n}=\sqrt{n}\frac{\overline{X}_{n}-\mu}{\sigma}\). Then
\begin{equation*}
  Z_{n} \overset{D}{\longrightarrow} Z\sim\mathcal{N}(0,1)
\end{equation*}

\section*{LLN}
Suppose \(\{X_{1},\ldots,X_{n}\} \overset{i.i.d.}{\sim} X\), with \(\mu = \E[X]\). Then
\begin{equation*}
  \overline{X}_{n} \overset{D}{\longrightarrow} \mu
\end{equation*}


\end{document}