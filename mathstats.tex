\documentclass[twocolumn]{amsart}
\usepackage[pdftex, a4paper, margin=0.7cm, nohead, nofoot]{geometry}
% \usepackage[utopia]{mathdesign}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{setspace}
\usepackage{scalefnt}
\usepackage{microtype}
\usepackage[absolute]{textpos}
\usepackage[compact]{titlesec}

\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\Beta}{\operatorname{Beta}}
\newcommand{\Cauchy}{\operatorname{Cauchy}}
\newcommand{\Unif}{\operatorname{Unif}}
\newcommand\iid{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny iid}}}{\sim}}}

% slight hack to make the independence symbol
% taken from here: https://tex.stackexchange.com/a/79436
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern3mu{#1#2}}}

\setlength{\parskip}{0pt}
\setlength{\parindent}{0pt}
\setlength{\TPHorizModule}{30mm}
\setlength{\TPVertModule}{\TPHorizModule}
\textblockorigin{10mm}{10mm} % start everything near the top-left corner
\titleformat{\section}{\centering\selectfont\bf}{}{0em}{}

\def\parsedate #1:20#2#3#4#5#6#7#8\empty{#6#7/#4#5/20#2#3\parsetime#8\empty}
\def\parsetime #1#2#3#4#5\empty{ #1#2:#3#4}
\def\moddate#1{\expandafter\parsedate\pdffilemoddate{#1}\empty}

% template taken from here: https://github.com/daleroberts/math-finance-cheat-sheet/

% More probability stuff
% https://wjgan.com/posts/latex.html

\begin{document}
\pagestyle{empty}
\thispagestyle{empty}
\setstretch{0.8}
\scalefont{0.8}

\begin{center}
\textbf{MathStats}
\vskip0.2em
\end{center}

\section*{Moments}
kth order moment
\begin{equation*}
  m_{k} = \E[X^{k}] = \int_{-\infty}^{\infty} x^k f_{X}(x) =
  \sum_{x:f_{X}(x)>0}x^{k} f_{X}(x)
\end{equation*}
kth order central moment
\begin{equation*}
  \sigma_{k} = \E[(X-m_{1})^{k}]
\end{equation*}


\section*{Expectation}
\begin{equation*}
  \E[g(X,Y)] = \sum_{y}\sum_{x}g(x,y)f_{X,Y}(x,y)
\end{equation*}
\begin{equation*}
  \E[Y] = \int_{-\infty}^{\infty}\E[Y|X=x]f_{X}(x)\,dx
\end{equation*}
If $X\geq 0$ (continuous or discrete)
\begin{equation*}
  \E[X] = \int_{0}^{\infty}(1-F_{X}(x))~dx
\end{equation*}
Conditional expectation
\begin{equation*}
  \E[Y|X=x] = \int y f_{y|x}(y|x)dy
\end{equation*}
\begin{equation*}
  \E[g(Y)|X=x] = \int g(y) f_{y|x}(y|x)dy
\end{equation*}
Law of iterated expectation
\begin{equation*}
  \E_{X}[E_{Y|X}\{g(Y)|X\}] = \E[g(Y)] = \sum_{x}\E[g(Y)|X=x]f_{X}(x)
\end{equation*}
\begin{equation*}
  \E[h(X)g(Y)] = \E[h(X)\E[g(Y)|X]]
\end{equation*}
\begin{equation*}
  \E[Y]=\E[\E[Y|X]] = \sum_{x}\E[Y|X = x]f_{X}(x)
\end{equation*}
If \(X \independent Y\), then
\begin{equation*}
  \E[g(Y) | X=x] = E[g(Y)]
\end{equation*}
\begin{equation*}
  \Var[X] = \E[X^{2}] - \E[X]^{2} = \Cov(X,X)
\end{equation*}
Special Case: Let $Y^{*} = I(Y=y)$
\begin{equation*}
  \E[Y^{*}] = \E[I(Y=y)] = \P(Y=y)
\end{equation*}
Special case cont
\begin{equation*}
  \E[Y^{*}] = \sum_{x}\E[Y^{*}|X=x]f_{X}(x) = \sum_{x}\P(Y=y|X=x)f_{X}(x) =
  \P(Y=y)
\end{equation*}

\section*{Basics}
\begin{equation*}
  \Cov[X,Y] = \E[(X-\E[X])(Y-\E[Y])] = \E[XY] - \E[X]\E[Y]
\end{equation*}
\begin{equation*}
  \rho(X,Y) = \frac{\Cov[X,Y]}{\sqrt{\Var[X]\cdot\Var[Y]}} = \frac{\Cov[X,Y]}{\sqrt{\sigma_{X}\sigma_{Y}}}
\end{equation*}

\section*{Expectation Algebra}
\begin{equation*}
  \E[X]=\E[X_{+}] - \E[X_{-}]~\text{if}~X_{+} \geq 0
\end{equation*}
\begin{equation*}
  \E[|X|] = \E[X_{+}] + \E[X_{-}]~\text{if}~X_{-} \geq 0
\end{equation*}
\begin{equation*}
  \E[a+bX] = a + b\E[X]
\end{equation*}
\begin{equation*}
  \E[aX+bY+c] = a\E[X]+b\E[Y]+c
\end{equation*}
If \(X \independent Y\), then
\begin{equation*}
  \E[XY] = \E[X]\E[Y]
\end{equation*}

\section*{Variance Algebra}
\begin{equation*}
  \Var[X + Y] = \Var[X] + 2\Cov[X, Y] + \Var[Y]
\end{equation*}
\begin{equation*}
  \Var[X - Y] = \Var[X] - 2\Cov[X, Y] + \Var[Y]
\end{equation*}
If \(X \independent Y\), then
\begin{equation*}
  \Var[X + Y] = \Var[X] + \Var[Y]
\end{equation*}
If \(X \independent Y\), then
\begin{equation*}
  \Var[X - Y] = \Var[X] - \Var[Y]
\end{equation*}
\begin{equation*}
  \Var[XY] = \E[X^2] \cdot \E[Y^2] - {(\E[X] \cdot \E[Y])}^2
\end{equation*}
\begin{equation*}
  \Var[X/Y] = \Var[X \cdot (1/Y)] = \Var[(1/Y) \cdot X]
\end{equation*}
\begin{equation*}
  \Var[aX+bY] = a^2\Var[X] + b^2 Var[Y] + 2ab\Cov[X,Y]
\end{equation*}

\section*{Correlation}
\begin{equation*}
  \Corr(X, Y) = \frac{\Cov(X, Y)}{\sqrt{\Var(X)}\sqrt{Var(Y)}}
\end{equation*}


\section*{Calc}
Chain rule
\begin{equation*}
  f(g(x))' = f'(g(x))g'(x)
\end{equation*}
Product rule
\begin{equation*}
  (u \cdot v)' = u' \cdot v + u \cdot v'
\end{equation*}
Quotient rule
\begin{equation*}
  \left(\frac{f(x)}{g(x)}\right)' = \frac{f'(x)g(x) - f(x)g'(x)}{g(x)^2}
\end{equation*}
Indeterminate Forms \\
$\frac{0}{0}$, $\frac{\pm\infty}{\pm\infty}$, $\infty-\infty$, $0*\infty$,
$0^{0}$, $1^{\infty}$, $\infty^{0}$ \\
Determinate Forms \\
$\infty+\infty=\infty$, $-\infty-\infty=-\infty$, $0^{\infty}=0$,
$0^{-\infty}=\infty$, $\infty*\infty=\infty$

\section*{Series}
Exponential
\begin{equation*}
  \sum_{n=0}^{\infty}\frac{x^{n}}{n!} = e^{x}
\end{equation*}
Geometric
\begin{equation*}
  \sum_{k=0}^{\infty}{ar^{k}}=\frac{a}{1-r}~\text{where}~|r|<1
\end{equation*}
\begin{equation*}
  \sum_{k=1}^{\infty}{kr^{k-1}}=\frac{1}{(1-r)^{2}}~\text{where}~|r|<1
\end{equation*}
\begin{equation*}
  \sum_{k=2}^{\infty}{k(k-1)r^{k-2}}=\frac{-2}{(1-r)^{3}}~\text{where}~|r|<1
\end{equation*}
Arithmetico-geometric
\begin{equation*}
  \sum_{k=1}^{\infty}k(1-p)^{k-1}=\frac{1}{p^{2}}=p^{-2}
\end{equation*}
\begin{equation*}
  \sum_{k=2}^{\infty}k(k-1)(1-p)^{k-2}=\frac{2}{p^{3}}
\end{equation*}
Binomial
\begin{equation*}
  (x+y)^{n} = \sum_{k=0}^{n}\binom{n}{k}x^{k}y^{n-k}
\end{equation*}
\begin{equation*}
  n(x+y)^{n-1} = \sum_{k=1}^{n}k\binom{n}{k}x^{k-1}y^{n-k}
\end{equation*}
\begin{equation*}
  n(n-1)(x+y)^{n-2} = \sum_{k=2}^{n}k(k-1)\binom{n}{k}x^{k-2}y^{n-k}
\end{equation*}


\section*{Binomial Coefficient}
\begin{equation*}
  \binom{n}{k} = \binom{n}{n-k}
\end{equation*}
\begin{equation*}
  \binom{n}{k} = \binom{n-1}{k-1} \left( \frac{n}{k} \right)~\text{if}~k > 0
\end{equation*}
% There is a fantastic webpage here with many more identities
% https://web.archive.org/web/20190629193344/http://www.math.wvu.edu/~gould/
% https://proofwiki.org/wiki/Factors_of_Binomial_Coefficient
\begin{equation*}
  (n+1)\binom{r}{n+1} = r\binom{r-1}{n}
\end{equation*}
\begin{equation*}
  \binom{n}{r+a}\binom{r+a}{r} = \binom{n}{r}\binom{n-r}{a}
\end{equation*}
\begin{equation*}
  \frac{1}{k+1}\binom{n}{k} = \binom{n+1}{k+1}\frac{1}{n+1}
\end{equation*}

\section*{Distribution Functions}
\begin{equation*}
  \lim_{x\to-\infty}F(x)=0~\lim_{x\to\infty}F(x)=1
\end{equation*}
\begin{equation*}
  F(x) \leq F(y)~\text{if}~x<y
\end{equation*}
\begin{equation*}
  \lim_{h \downarrow 0}F(x+h)=F(x)
\end{equation*}
\begin{equation*}
  F(x)=\int_{-\infty}^{x}f(t)~dt~\text{where}~f(t)\geq
  0~\text{and}\int_{-\infty}^{\infty}f(t)~dt = 1
\end{equation*}
\begin{equation*}
  F_{X,Y}(x,y)=\P(X\leq x,Y\leq y)
\end{equation*}
\begin{equation*}
  F_{\boldsymbol{X}}(\boldsymbol{x})=\P(\boldsymbol{X}\leq\boldsymbol{x}) =
  \P(X_{1}\leq x_{1},\ldots,X_{k}\leq x_{k})
\end{equation*}
\begin{equation*}
  F_{\boldsymbol{X}}(\boldsymbol{x})=\int_{-\infty}^{x_{1}}\dotsi
  \int_{-\infty}^{x_{k}}f(t_{1},\cdots,t_{k})~dt_{1}\dotsm dt_{k}
\end{equation*}
\begin{equation*}
  \P(a < X \leq b, c < Y \leq d) = F(b,d) - F(a,d) - F(b,c) + F(a,c)
\end{equation*}
Joint Density
\begin{equation*}
  F(x,y) = \P(X \leq x,Y \leq y) = \int_{-\infty}^{x}\left\{ \int_{-\infty}^{y}
    f(u,v)~dv\right\}~du
\end{equation*}

\section*{Random Variables}
\begin{equation*}
  X: \Omega \rightarrow \mathbb{R}
\end{equation*}
\begin{equation*}
  \{\omega \in \Omega : X(\omega) \leq x\} \in \mathcal{F}
\end{equation*}
\begin{equation*}
  F(x) = \P(X \leq x) = \P(\{\omega : X(\omega) \leq x\})
\end{equation*}
\begin{equation*}
  \P(X>x) = 1-F(x)
\end{equation*}
\begin{equation*}
  \P(x<X\leq y) = F(y) - F(x)
\end{equation*}
\begin{equation*}
  \P(X<x) = \lim_{n\to \infty}F(x - \frac{1}{n})
\end{equation*}
\begin{equation*}
  \P(X=x) = \P(X\leq x) - \P(X < x) = F(x)-\lim_{n\to \infty}F(x-\frac{1}{n})
\end{equation*}

\section*{PDF}
Conditional PDF
\begin{equation*}
  f_{Y|X}(y|x) = \frac{f(x,y)}{f_{X}(x)} =
  \frac{f(x,y)}{\int_{-\infty}^{\infty}f(x,y)\,dy} = \int_{-\infty}^{y} \frac{f(x,v)}{f_{X}(x)}\,dv
\end{equation*}
Joint PDF
\begin{equation*}
  f(x,y) = \frac{\partial^{2}}{\partial y \partial x}F(x,y)
\end{equation*}
Marginal PDF
\begin{equation*}
  f_{X}(x) = \int_{-\infty}^{+\infty} f(x,v)~dv,~f_{Y}(y) = \int_{-\infty}^{+\infty} f(u,y)~du
\end{equation*}

\section*{Convolution}
Let $Z=X+Y$
\begin{equation*}
  f_{Z}(z)=\sum_{x}f_{X}(x)f_{Y|X}(z-x|x) = \sum_{y}f_{Y}(y)f_{X|Y}(z-y|y)
\end{equation*}
If \(X \independent Y\), then
\begin{equation*}
  f_{Z}(z)=\sum_{x}f_{X}(x)f_{Y}(z-x) = \sum_{y}f_{Y}(y)f_{X}(z-y)
\end{equation*}

\begin{equation*}
  f_{Z}(z)=\int_{-\infty}^{\infty}f_{X,Y}(x,z-x)~dx = \int_{-\infty}^{\infty}f_{X,Y}(z-y,y)~dy
\end{equation*}
If \(X \independent Y\), then
\begin{equation*}
  f_{Z}(z)=\int_{-\infty}^{\infty}f_{X}(x)f_{Y}(z-x)~dx = \int_{-\infty}^{\infty}f_{X}(z-y)f_{Y}(y)~dy
\end{equation*}

\section*{Bivariate Normal}
\begin{equation*}
  \Sigma = \Cov(\boldsymbol{X},\boldsymbol{X})
  = \begin{pmatrix}
    \sigma_{1}^{2}&\Cov(X_{1},X_{2})\\
      \Cov(X_{1},X_{2})&\sigma_{2}^{2}
  \end{pmatrix}
  = \begin{pmatrix}
    \sigma_{1}^{2}&\rho\sigma_{1}\sigma_{2}\\
      \rho\sigma_{1}\sigma_{2}&\sigma_{2}^{2}
  \end{pmatrix}
\end{equation*}

% \begin{equation*}
%   f_{\boldsymbol{X}}(\boldsymbol{x}) =
%   \frac{1}{2\pi\sigma_{1}\sigma{2}\sqrt{1-\rho^{2}}}
%   \exp\left\{-\frac{1}{2(1-\rho^{2})}\left( \frac{x_{1} - \mu_{1}}{\sigma_{1}} \right)^{2}
%     - 2\rho \left(\frac{x_{1} - \mu_{1}}{\sigma_{1}} \frac{x_{2} -
%         \mu_{2}}{\sigma_{2}}\right)
%     + \left(\frac{x_{2} - \mu_{2}}{\sigma_{2}}\right)^{2} \right\}
% \end{equation*}

\begin{equation*}
  \Cov(X_{1},X_{2}) = 0 \iff X_{1} \independent X_{2}
\end{equation*}

\begin{equation*}
  \boldsymbol{U} = \boldsymbol{v} + C\boldsymbol{X} \sim
  \mathcal{N}(\boldsymbol{v} + C\boldsymbol{\mu},C\Sigma C)
\end{equation*}

\begin{equation*}
  X_{2} | X_{1} = x_{1} \sim \mathcal{N}(\mu_{2} + \rho
  \frac{\sigma_{2}}{\sigma{1}}(x_{1} - \mu_{1}),\sigma_{2}^{2}(1-\rho^{2}))
\end{equation*}

\begin{equation*}
  \E[X_{2} | X_{1}] = \mu_{2} + \rho \frac{\sigma_{2}}{\sigma{1}}(X_{1} - \mu_{1})
\end{equation*}


\section*{Distributions arising from $\mathcal{N}$}
Gamma distribution
\begin{equation*}
  \Gamma\left(\frac{1}{2},\frac{k}{2}\right) \rightarrow \chi_{k}^{2}
  \rightarrow t_{k} \stackrel{D}{=}
  \frac{\mathcal{N}(0,1)}{\sqrt{\chi_{k}^{2}/k}} \rightarrow F_{m,k}
  \stackrel{D}{=} \frac{\chi_{m}^{2}/m}{\chi_{k}^{2}/k}.
  (F_{1,k} \stackrel{D}{=} t_{k}^{2})
\end{equation*}

\begin{equation*}
  X\sim\Gamma(\lambda,p) \iff \lambda X \sim \Gamma(1,p)
  \stackrel{c>0}{\implies} cX\sim \Gamma(\frac{\lambda}{c},p)
\end{equation*}

\begin{equation*}
  X_{j}\sim \Gamma(\lambda, p_{j}), j=1,\ldots,k, X_{1},\ldots,X_{k}
  \independent \implies \sum_{j=1}^{k}X_{j} \sim \Gamma(\lambda, \sum_{j=1}^{k}p_{j})
\end{equation*}

\begin{equation*}
  \begin{cases}
    X_{1} & \sim \Gamma(\lambda, p) \\
    X_{2} & \sim \Gamma(\lambda, q) \\
    X_{1} & \independent X_{2}
  \end{cases}
  \implies
  \begin{cases}
    Y_{1} = X_{1} + X_{2} & \sim \Gamma(\lambda, p + q) \\
    Y_{2} = \frac{X_{1}}{X_{1} + X_{2}} & \sim \Beta(p, q) \\
    Y_{1} & \independent Y_{2}
  \end{cases}
\end{equation*}

$\chi_{k}^{2}$ distribution (k degrees of freedom)

\begin{equation*}
  Z_{1},\ldots,Z_{k}\iid\mathcal{N}(0,1) \implies V =
  Z^{2}_{1}+\cdots+Z^{2}_{k} \sim \chi_{k}^{2}
\end{equation*}

\begin{equation*}
  f_{V}(v) = \frac{v^{(k-2)/2}e^{-v/2}}{2^{k/2}\Gamma(k/2)} \sim
  \Gamma\left(\frac{1}{2},\frac{k}{2}\right), v \geq 0
\end{equation*}

\begin{equation*}
  \mathcal{N}(0,1)^{2} \sim \chi_{1}^{2} \stackrel{D}{=} \Gamma\left(\frac{1}{2},\frac{1}{2}\right)
\end{equation*}

\begin{equation*}
  \chi_{2}^{2} = \Gamma\left(\frac{1}{2},1\right) = \Exp\left(\frac{1}{2}\right)
\end{equation*}

$t_{k}$ distribution (k degrees of freedom)\\
if
\begin{equation*}
  Z \sim \mathcal{N}(0, 1),~V \sim \chi_{k}^{2},~Z \independent V
\end{equation*}
then
\begin{equation*}
  Q = \frac{Z}{\sqrt{V/k}} \sim t_{k}
\end{equation*}
\begin{equation*}
  f_{Q}(q) = c_{1}{(1+\frac{q^{2}}{k})}^{-(k+1)/2}, q \in \mathbb{R}
\end{equation*}
where \(c_{1}>0\) is a constant
\begin{equation*}
  \frac{1}{1 + q^{2}} \sim \Cauchy(0,1) \stackrel{D}{=} t_{1}
\end{equation*}

\begin{equation*}
  t_{k} \to \mathcal{N}(0,1) \text{ as } k\to\infty
\end{equation*}

\textbf{F Distribution} \\
if
\begin{equation*}
  V \sim \chi^{2}_{m},~W \sim \chi^{2}_{k},~V \independent W
\end{equation*}
then
\begin{equation*}
  S = \frac{V/m}{W/k} \sim F_{m,k}
\end{equation*}
\begin{equation*}
  f_{S}(s) = c_{2}s^{(m-2)/2}(1+\frac{m}{k}s)^{-(k+m)/2},s>0
\end{equation*}
where $c_{2} > 0$ is a constant

Special case:
if \(m=1\), then
\begin{equation*}
  F_{m,k}=F{1,k} \stackrel{D}{=} t^{2}_{k}
\end{equation*}

\section{Estimators}
Estimate $\mu$
\begin{equation*}
  \overline{X}_{n} = \frac{1}{n}\sum_{i=1}^{n}X_{i}
\end{equation*}
Estimate $\sigma^{2}$
\begin{equation*}
  S_{n}^{2} = \frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X}_{n})^{2}
\end{equation*}
Suppose $\{X_{1},\ldots,X_{n}\}\iid\mathcal{N}(\mu,\sigma^{2})$, then
\begin{equation*}
  \sqrt{n}\frac{\overline{X}_{n} - \mu}{\sigma}\sim\mathcal{N}(0,1),~~
  \overline{X}_{n}\independent S_{n}^{2},~~
  \E[S_{n}^{2}] = \sigma^{2},~~\frac{(n-1)S_{n}^{2}}{\sigma^{2}}\sim \chi^{2}_{n-1}
\end{equation*}
And
\begin{equation*}
  \sqrt{n}\frac{\overline{X}_{n} - \mu}{\sigma}\sim \mathcal{N}(0,1),~~
  \sqrt{n}\frac{\overline{X}_{n} - \mu}{S_{n}}\sim t_{n-1}
\end{equation*}

\section{Sampling}
Inverse Transform

Let $U\sim\Unif(0, 1)$, $F$ be some continuous distribution. Assume $F(x)$ is strictly
$\uparrow$ in x, then: $X=F^{-1}(U)\sim F$

Discrete

Let $U\sim\Unif(0, 1)$, suppose $X\in\{0,1,2,\ldots\} \sim F(x)$. We wish to
generate X. For $k=0,1,2,\ldots$, define $Y=k\iff F(k-1)<U\leq F(k)$, then
$Y\sim F$

Rejection

Suppose $X\sim F_{X}$ with a known PDF: $f_{X}$. Let $U\sim\Unif(0,1)$,
$Z\independent U$, $Z\sim f_{Z}(z)$, where the pdf is known. $\exists$ a known
constant $a > 0$, such that $f_{X}(z)\leq a f_{Z}(z) \forall z$

Event $E=\{aUf_{Z}(Z)\leq f_{X}(Z)\}$. $P(Z\leq x | E) = F_{X})(x)$


\section{Generating Functions -- PGF}
(non-negative integer-valued RVs) -- No finiteness guarantee
\begin{equation*}
  G_{X}(s) = \E(s^{X}) = \sum_{j=0}^{\infty}s^{j}\P(X=j) = \sum_{j=0}^{\infty}s^{j}f_{X}(j)
\end{equation*}
Factorial moments
\begin{equation*}
  \E[X(X-1)\cdots(X-k+1)] = G^{(k)}(1)
\end{equation*}

\begin{equation*}
  X \independent Y \implies G_{X+Y}(s) = G_{X}(s)G_{Y}(s)
\end{equation*}

If $X_{1},X_{2},\ldots$ iid  with PGF $G_{X}$ and $N (\geq 0)$ is a rv w/ PGF
$G_{N}$, then the sum $S_{N} = X_{1}+\cdots+X_{N}$ has the PGF:
\begin{equation*}
  G_{S_{N}}(s) = G_{N}(G_{X}(t))
\end{equation*}
\begin{equation*}
  G_{X_{1},X_{2}}(s_{1},s_{2}) = \E[s_{1}^{X_{1}}s_{2}^{X_{2}}]
\end{equation*}
If $X_{1} \independent X_{2}$:
\begin{equation*}
  G_{X_{1},X_{2}}(s_{1},s_{2}) = G_{X_{1}}(s_{1})G_{X_{2}}(s_{2})
\end{equation*}


\section{Generating Functions -- MGF}
(General RVs)
\begin{equation*}
  M_{X}(t) = \E(e^{tX}) = G_{X}(e^{t}) = \sum_{j=0}^{\infty}e^{tj}f_{X}(j) =
  \int e^{tx}f_{X}(x)~dx
\end{equation*}
\begin{equation*}
  M_{X_{1},X_{2}}(t_{1},t_{2}) = \E[e^{t_{1}X_{1}}e^{t_{2}X_{2}}] = G_{X_{1},X_{2}}(e^{t_{1}},e^{t_{2}})
\end{equation*}
If $X_{1} \independent X_{2}$:
\begin{equation*}
  M_{X_{1},X_{2}}(t_{1},t_{2}) = M_{X_{1}}(t_{1})M_{X_{2}}(t_{2})
\end{equation*}
For $S_{n} = X_{1}+\cdots+X_{n}$, if $\left\{X_{i}\right\}^{n}_{i=1}$ are
independent, then:
\begin{equation*}
  M_{S_{n}}(t) = \prod_{k=1}^{n}M_{X_{k}}(t)
\end{equation*}
In addition, if $\left\{X_{i}\right\}^{n}_{i=1}$ have the same distribution as
X, then:
\begin{equation*}
  M_{S_{n}}(t) = (M_{X}(t))^{n}
\end{equation*}

For $Y=a+bX$, if $M_{X}$ exists:
\begin{equation*}
  M_{Y}(t) = \E[e^{tY}] = \E[e^{ta+tbX}] = e^{ta}\E[e^{tbX}] = e^{ta}M_{X}(tb)
\end{equation*}

If $|M_{X}(t)| < \infty$ for $t \in (-a,a)$, where $a>0$ is some constant, then:
\begin{equation*}
  M_{X}^{(k)}(0) = \E[X^{k}]
\end{equation*}

\section*{Stein's Lemma}
If \(g(z)\) is differentiable and \(\lim\limits_{z\to-\infty} g(z)\phi(z) = 0\) and \(\lim\limits_{z\to+\infty} g(z)\phi(z) = 0\) \\
Then for \(Z\sim\mathcal{N}(0, 1)\): $\E[Z g(Z)] = \Cov(Z, g(Z)) = \E[g'(Z)]$

\textbf{Applications}\\
For \(Z\sim\mathcal{N}(0,1)\): $\E[Z^{2k}] = (2k - 1)!!$
if \(\lim\limits_{z\to\pm\infty} m(\mu + \sigma z)\phi(z)=0\), then: $Cov(X,m(X)) = \sigma^{2}\E[m'(X)]$

\section*{CLT}
Suppose \(\{X_{1},\ldots,X_{n}\} \iid X\), with \(\mu = \E[X]\) and \(\sigma^{2} = \Var(X) \in (0, \infty)\). Define \(Z_{n}=\sqrt{n}\frac{\overline{X}_{n}-\mu}{\sigma}\). Then
\begin{equation*}
  Z_{n} \overset{D}{\longrightarrow} Z\sim\mathcal{N}(0,1)
\end{equation*}

\section*{LLN}
Suppose \(\{X_{1},\ldots,X_{n}\} \iid X\), with \(\mu = \E[X]\). Then
\begin{equation*}
  \overline{X}_{n} \overset{D}{\longrightarrow} \mu
\end{equation*}

\section*{MGFs}
Gamma: $\Gamma(\lambda,\alpha) =
\left(\frac{\lambda}{\lambda-t}\right)^{\alpha}, t < \lambda$

Binomial: $(q+pe^{t})^{n}$

Poisson: $\exp\{\lambda(e^{t} - 1)\}$

Geometric: $\frac{pe^{t}}{1-(1-p)e^{t}}, t<-\ln(1-p)$


\end{document}